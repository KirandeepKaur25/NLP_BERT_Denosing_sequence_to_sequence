# -*- coding: utf-8 -*-
"""Final Project  (BART)nlp student id -1032****.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMhBkhgtvevvPNqBtkVu0GG1xde4fUWp

NATURAL LANGUAGE PROCESSING

CODE-COMP 650

PROJECT NAME-  BART DENOISING SEQUENCE-TO-SEQUENCE

STUDENT NAME- KIRANDEEP KAUR STUDENT ID- 10325...
"""

!nvidia-smi # Checking GPU

!pip install transformers # Installing the transformers library (https://huggingface.co/docs/transformers/index)

!pip install datasets # Installing the datasets library (https://huggingface.co/docs/datasets/index)

!pip install evaluate # Installing the evaluate library (https://huggingface.co/docs/evaluate/main/en/index)

!pip install rouge-score # Installing rouge-score library (https://pypi.org/project/rouge-score/)

!pip install py7zr # Installing library to save zip archives (https://pypi.org/project/py7zr/)

# Importing Libraries

# Data Handling
import pandas as pd
import numpy as np
from datasets import Dataset, load_metric
import shutil

# Data Visualization
import plotly.express as px
import plotly.graph_objs as go
import plotly.subplots as sp
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
import plotly.io as pio
from IPython.display import display
from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)

# Statistics & Mathematics
import scipy.stats as stats
import statsmodels.api as sm
from scipy.stats import shapiro, skew, anderson, kstest, gaussian_kde,spearmanr
import math

# Hiding warnings
import warnings
warnings.filterwarnings("ignore")

# Transformers
from transformers import BartTokenizer, BartForConditionalGeneration      # BERT Tokenizer and architecture
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments         # These will help us to fine-tune our model
from transformers import pipeline                                         # Pipeline
from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data
import torch                                                              # PyTorch
import evaluate                                                           # Hugging Face's library for model evaluation


# Other NLP libraries
from textblob import TextBlob                                             # This is going to help us fix spelling mistakes in texts
from sklearn.feature_extraction.text import TfidfVectorizer               # This is going to helps identify the most common terms in the corpus
import re                                                                 # This library allows us to clean text data
import nltk                                                               # Natural Language Toolkit
nltk.download('punkt')                                                    # This divides a text into a list of sentences

"""> <p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">By observing the imports above, you can clearly note that I have choosen to work with <b>PyTorch</b> for this notebook.</p>
"""

# Configuring Pandas to exhibit larger columns
'''
This is going to allow us to fully read the dialogues and their summary
'''
pd.set_option('display.max_colwidth', 1000)

# Configuring notebook
seed = 42
#paper_color =
#bg_color =
colormap = 'cividis'
template = 'plotly_dark'

# Checking if GPU is available
if torch.cuda.is_available():
    print("GPU is available. \nUsing GPU")
    device = torch.device('cuda')
else:
    print("GPU is not available. \nUsing CPU")
    device = torch.device('cpu')

"""<div id = 'eda'
     style="font-family: Calibri, serif; text-align: left;">
    <hr style="border: none;
               border-top: 2.85px solid #041445;
               width: 100%;
               margin-top: 62px;
               margin-bottom: auto;
               margin-left: 0;">
    <div style="font-size: 56px; letter-spacing: 2.25px;color: #02011a;"><b>Exploring the Dataset</b></div>
</div>

Load the Three sets available train,test and val
"""

import zipfile
import os

# Replace 'your_file.zip' with the name of your uploaded zip file
zip_file_path = '/content/archive (8).zip'
extract_dir = 'extracted_files'

# Create a directory to extract the files
os.makedirs(extract_dir, exist_ok=True)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extracted directory
extracted_files = os.listdir(extract_dir)
print("Extracted files:", extracted_files)

# Loading data
train = pd.read_csv('/content/extracted_files/samsum-train.csv')
test = pd.read_csv('/content/extracted_files/samsum-test.csv')
val = pd.read_csv('/content/extracted_files/samsum-validation.csv')

"""Analyse the Train, test and Validation dataset individuallty

<div id = 'train'
     style="font-family: Calibri, serif; text-align: left;">
    <hr style="border: none;
               width: 100%;
               margin-top: 62px;
               margin-bottom: auto;
               margin-left: 0;">
    <div style="font-size: 32px; letter-spacing: 2.25px;color: #02011a;"><b>Train Dataset</b></div>
</div>
"""

# Extracting info on the training Dataframe
describe_df(train)

"""Dataset contain 14,732 pairs of dialogues and summaries.

Find the Null dialogue
"""

mask = train['dialogue'].isnull() # Creating mask with null dialogues
filtered_train = train[mask] # filtering dataframe
filtered_train # Visualizing

"""6054 does not really add anything to the dataset. consider it as  a Null dialogue"""

train = train.dropna() # removing null values

# Removing 'Id' from categorical features list
categorical_features.remove('id')

"""<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">We can now analyze the length of both dialogues and summaries by counting the words in them. This might give us a clue about how these texts are structured.</p>
"""

print(df_text_lenght.head())

"""On average, dialogues consist of about 94 words. We do have some outliers with very extensive texts, going way over 300 words per dialogue.
Summaries are naturally shorter texts, consisting of about 20 words on average, although we also have some outliers with extensive summaries.

Filter Dataset to see those containig the term '15 minutes' in the summary
"""

# Filtering dataset to see those containing the term '15 minutes' in the summary
filtered_train = train[train['summary'].str.contains('15 minutes', case=False, na=False)]
filtered_train.head()

"""Here, in dialogues we can see that in chat 15min and 15m are written, in summaries give us a patternized description like 15 minutes.

<div id = 'test'
     style="font-family: Calibri, serif; text-align: left;">
    <hr style="border: none;
               width: 100%;
               margin-top: 62px;
               margin-bottom: auto;
               margin-left: 0;">
    <div style="font-size: 32px; letter-spacing: 2.25px;color: #02011a;"><b>Test Dataset</b></div>
</div>
"""

# Extracting info on the test dataset
describe_df(test)

# Removing 'Id' from categorical features list
categorical_features.remove('id')

"""<div id = 'val'
     style="font-family: Calibri, serif; text-align: left;">
    <hr style="border: none;
               width: 100%;
               margin-top: 62px;
               margin-bottom: auto;
               margin-left: 0;">
    <div style="font-size: 32px; letter-spacing: 2.25px;color: #02011a;"><b>Validation Dataset</b></div>
</div>
"""

# Extracting info on the val dataset
describe_df(val)

# Removing 'Id' from categorical features list
categorical_features.remove('id')

"""<div id = 'preprocess'
     style="font-family: Calibri, serif; text-align: left;">
    <hr style="border: none;
               border-top: 2.85px solid #041445;
               width: 100%;
               margin-top: 62px;
               margin-bottom: auto;
               margin-left: 0;">
    <div style="font-size: 56px; letter-spacing: 2.25px;color: #02011a;"><b>Preprocessing Data</b></div>
</div>
"""

print(train['dialogue'].iloc[14727])

print(test['dialogue'].iloc[0])

"""clean_tags function defined below to remove these tags from the texts,to make them cleaner."""

def clean_tags(text):
    clean = re.compile('<.*?>') # Compiling tags
    clean = re.sub(clean, '', text) # Replacing tags text by an empty string

    # Removing empty dialogues
    clean = '\n'.join([line for line in clean.split('\n') if not re.match('.*:\s*$', line)])

    return clean

test1 = clean_tags(train['dialogue'].iloc[14727]) # Applying function to example text
test2 = clean_tags(test['dialogue'].iloc[0]) # Applying function to example text

# Printing results
print(test1)
print('\n' *3)
print(test2)

""" successfully removed the tags from the texts."""

# Defining function to clean every text in the dataset.
def clean_df(df, cols):
    for col in cols:
        df[col] = df[col].fillna('').apply(clean_tags)
    return df

# Cleaning texts in all datasets
train = clean_df(train,['dialogue', 'summary'])
test = clean_df(test,['dialogue', 'summary'])
val = clean_df(val,['dialogue', 'summary'])

train.tail(3) # Visualizing results

"""<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">The tags have been removed from the texts. It's beneficial to conduct such data cleansing to eliminate noise—information that might not significantly contribute to the overall context and could potentially impair performance.</p>

convert our Pandas Dataframes to Datasets to make our data ready to be processed across the whole Hugging Face ecosystem.
"""

# Transforming dataframes into datasets
train_ds = Dataset.from_pandas(train)
test_ds = Dataset.from_pandas(test)
val_ds = Dataset.from_pandas(val)

# Visualizing results
print(train_ds)
print('\n' * 2)
print(test_ds)
print('\n' * 2)
print(val_ds)

""" select a specific row"""

train_ds[0] # Visualizing the first row

"""<div id = 'modeling'
     style="font-family: Calibri, serif; text-align: left;">
    <hr style="border: none;
               border-top: 2.85px solid #041445;
               width: 100%;
               margin-top: 62px;
               margin-bottom: auto;
               margin-left: 0;">
    <div style="font-size: 56px; letter-spacing: 2.25px;color: #02011a;"><b>Modeling</b></div>
</div>
"""

# Loading summarization pipeline with the bart-large-cnn model
summarizer = pipeline('summarization', model = 'facebook/bart-large-xsum')

news = '''Bobi, the world’s oldest dog ever, has died after reaching the almost inconceivable age of 31 years and 165 days, said Guinness World Records (GWR) on Monday.
His death at an animal hospital on Friday was initially announced by veterinarian Dr. Karen Becker.
She wrote on Facebook that “despite outliving every dog in history, his 11,478 days on earth would never be enough, for those who loved him.”
There were many secrets to Bobi’s extraordinary old age, his owner Leonel Costa told GWR in February. He always roamed freely, without a leash or chain, lived in a “calm, peaceful” environment and ate human food soaked in water to remove seasonings, Costa said.
He spent his whole life in Conqueiros, a small Portuguese village about 150 kilometers (93 miles) north of the capital Lisbon, often wandering around with cats.
Bobi was a purebred Rafeiro do Alentejo – a breed of livestock guardian dog – according to his owner. Rafeiro do Alentejos have a life expectancy of about 12-14 years, according to the American Kennel Club.
But Bobi lived more than twice as long as that life expectancy, surpassing an almost century-old record to become the oldest living dog and the oldest dog ever – a title which had previously been held by Australian cattle-dog Bluey, who was born in 1910 and lived to be 29 years and five months old.
However, Bobi’s story almost had a different ending.
When he and his three siblings were born in the family’s woodshed, Costa’s father decided they already had too many animals at home.
Costa and his brothers thought their parents had taken all the puppies away to be destroyed. However, a few sad days later, they found Bobi alive, safely hidden in a pile of logs.
The children hid the puppy from their parents and, by the time Bobi’s existence became known, he was too old to be put down and went on to live his record-breaking life.
His 31st birthday party in May was attended by more than 100 people and a performing dance troupe, GWR said.
His eyesight deteriorated and walking became harder as Bobi grew older but he still spent time in the backyard with the cats, rested more and napped by the fire.
“Bobi is special because looking at him is like remembering the people who were part of our family and unfortunately are no longer here, like my father, my brother, or my grandparents who have already left this world,” Costa told GWR in May. “Bobi represents those generations.”
'''
summarizer(news) # Using the pipeline to generate a summary of the text above

checkpoint = 'facebook/bart-large-xsum' # Model
tokenizer = BartTokenizer.from_pretrained(checkpoint) # Loading Tokenizer

model = BartForConditionalGeneration.from_pretrained(checkpoint) # Loading Model

"""<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">We can also print below the architecture of the model.</p>
"""

print(model) # Visualizing model's architecture

"""It is possible to see that the models consist of an encoder and a decoder, we can see the Linear Layers, as well as the activation functions, which use GeLU, ReLU.
          
It is also interesting to observe the output layer, lm_head, which shows us that this model is ideal for generating outputs with a vocabulary size

<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">Now we must preprocess our datasets and use BartTokenizer so that our data is legible for the BART model.</p>

<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">The following <code>preprocess_function</code> can be directly copied from the 🤗 Transformers documentation, and it serves well to preprocess data for several NLP tasks. I am going to delve a bit deeper into how it preprocesses the data by explaining the steps it takes.</p>

<div style = "margin-left: 25px;">
    
<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a"><b>• <code>inputs = [doc for doc in examples["dialogue"]]:</code></b> In this line, we are iterating over every <code>dialogue</code> in the dataset and saving them as input to the model.</p>
    
<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a"><b>• <code>model_inputs = tokenizer(inputs, max_length=1024, truncation=True)
:</code></b> Here, we are using the <code>tokenizer</code> to convert the input dialogues into tokens that can be easily understood by the BART model. The <code>truncation=True</code> parameter ensures that all dialogues have a maximum number of 1024 tokens, as defined by the <code>max_length</code> parameter.</p>
    
<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a"><b>• <code>labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True):</code></b> This line performs a very similar tokenization process as the one above. This time, however, it tokenizes the target variable, which is our summaries. Also, note that the max_length here is significantly lower, at 128. This implies that we expect summaries to be a much shorter text than that of dialogues.</p>
    
<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a"><b>• <code>model_inputs["labels"] = labels["input_ids"]:</code></b> This line is essentially adding the tokenized labels to the preprocessed dataset, alongside the tokenized inputs.</p>
  
</div>
"""

def preprocess_function(examples): #
    inputs = [doc for doc in examples["dialogue"]] #  iterating over every dialogue in the document in the dataset  and save in model.
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True) # to convert input dialogue in to tokens

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=128, truncation=True) # to convert summary to short dialogue

    model_inputs["labels"] = labels["input_ids"]  # adding labels to the token
    return model_inputs

# Applying preprocess_function to the datasets
tokenized_train = train_ds.map(preprocess_function, batched=True,
                               remove_columns=['id', 'dialogue', 'summary', '__index_level_0__']) # Removing features

tokenized_test = test_ds.map(preprocess_function, batched=True,
                               remove_columns=['id', 'dialogue', 'summary']) # Removing features

tokenized_val = val_ds.map(preprocess_function, batched=True,
                               remove_columns=['id', 'dialogue', 'summary']) # Removing features

# Printing results
print('\n' * 3)
print('Preprocessed Training Dataset:\n')
print(tokenized_train)
print('\n' * 2)
print('Preprocessed Test Dataset:\n')
print(tokenized_test)
print('\n' * 2)
print('Preprocessed Validation Dataset:\n')
print(tokenized_val)

"""Our tokenized datasets consist now of only three features, nput_ids, <code>attention_mask, and labels. Let's print a sample from our tokenized train dataset to investigate further how the preprocess function altered the data."""

# Selecting a sample from the dataset
sample = tokenized_train[0] # Visualiz the first sample

# Printing its features
print("input_ids:")
print(sample['input_ids'])
print("\n")
print("attention_mask:")
print(sample['attention_mask'])
print("\n")
print("sample:")
print(sample['labels'])
print("\n")

"""input_ids- the token IDs mapped to the dialogues. Each token represents a word or subword that can be perfectly understood by the BART model. For instance, the number could be a map to a word like "hello" in BART's vocabulary. Each word has its unique token in this context.
    
attention_mask: tokens the model should pay attention to and which tokens should be ignored. tokens are masked as '1', meaning they are all relevant
    
labels:  these are token IDs obtained from the words and subwords in the summaries. These are the tokens that the model will be trained on to give as output.

use DataCollatorForSeq2Seq to batch the data. These data collators may also automatically apply some processing techniques, such as padding. They are important for the task of fine-tuning models and are also present in the 🤗 Transformers documentation for text summarization.
"""

# Instantiating Data Collator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

metric = load_metric('rouge') # Loading ROUGE Score

def compute_metrics(eval_pred): # extracting the model-generated summaries, human-generated summaries, and decoding them
    predictions, labels = eval_pred# Obtaining predictions and true labels

    # Decoding predictions
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label = -100)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]


    # Computing rouge score
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} # Extracting some results

    # Add mean-generated length
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}

# Defining parameters for training or train a sequence-to-sequence model like BART, balancing training speed, model performance, and resource usage.

training_args = Seq2SeqTrainingArguments(
    output_dir = 'bart_samsum',
    evaluation_strategy = "epoch",
    save_strategy = 'epoch',
    load_best_model_at_end = True,
    metric_for_best_model = 'eval_loss',
    seed = seed,
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16=True,
    report_to="none"
)

"""<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">Finally, the <code>Seq2SeqTrainer</code> class allows us to use <b>PyTorch</b> to fine-tune the model. In this class, we are basically defining the model, the training arguments, the datasets used for training and evaluation, the tokenizer, the data_collator, and the metrics.</p>

ROUGE-L: It measures the Longest Common Subsequence (LCS) between the two summaries, which helps to capture content coverage of the machine-generated text. If both summaries have the sequence "the apple is green", we have a match regardless of where they appear in both texts.

• ROUGE-S: It evaluates the overlap of skip-bigrams, which are bigrams that permit gaps between words. This helps to measure the coherence of a machine-generated summary. For example, in the phrase "this apple is absolutely green", we find a match for the terms such as "apple" and "green", if that is what we are looking for.
"""

# Defining Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train() # Training model

"""We finally finished fine-tuning after 4 epochs. Since  we had load_best_model_at_end= True in the training arguments, the Trainer automatically saves the model with the best performance, which in this case is the one with the lowes Validation Loss.
The Second Epoch was the one with the lowest vaidation loss,at 1.5576 . It achieved the highest Rouge1 and Roug2 scores, as well as the highest Rougelsum Score.
The GenLen Column give us the average length of the model-generated summaries
Second epoch shows the shortes summaries

##Evaluating and saving the model

After training and testing the model, we can evaluate its performance on the validation  dataset.
"""

# Evaluating model performance on the tokenized validation dataset
validation = trainer.evaluate(eval_dataset = tokenized_val)
print(validation)

"""The outputs is same as seen in previous training and testing and have higher performance in every metric compared to the performance in the training set.

Use the Save_model method to save our fine_tuned model in the bart_finetuned_samsum directory.
"""

# Saving model to a custom directory
directory = "bart_finetuned_samsum"
trainer.save_model(directory)

# Saving model tokenizer
tokenizer.save_pretrained(directory)

"""Use the Shutil package to save the model in zip"""

# Saving model in .zip format
shutil.make_archive('bart_finetuned_samsum', 'zip', '/content/bart_finetuned_samsum')
shutil.move('bart_finetuned_samsum.zip', '/content/bart_finetuned_samsum')

"""Upload the Hugging face model and use it on a new dataset and texts. https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fluisotorres%2Fbart-finetuned-samsum and even access it

Load the model, in summarization pipeline, and generate some summaries for evaluation,the accuracy
"""

# Loading summarization pipeline and model
summarizer = pipeline('summarization', model = 'luisotorres/bart-finetuned-samsum')

"""<p style="font-family: Calibri, serif; text-align: left;
          font-size: 24px; letter-spacing: .85px;color: #02011a">After loading the pipeline, we can now produce some summaries. I'll first start by using examples from the validation dataset, so we can compare our model-generated summaries to the reference summaries.</p>

Summaries, by using validation datasets, to compare our model generated summaries to reference summaries.
"""

# Obtaining a random example from the validation dataset
val_ds[35]

text = "John: doing anything special?\r\nAlex: watching 'Millionaires' on tvn\r\nSam: me too! He has a chance to win a million!\r\nJohn: ok, fingers crossed then! :)"
summary = "Alex and Sam are watching Millionaires."
generated_summary = summarizer(text)

print('Original Dialogue:\n')
print(text)
print('\n' * 2)
print('Reference Summary:\n')
print(summary)
print('\n' * 2)
print('Model-generated Summary:\n')
print(generated_summary)

"""The model-generated summary is just a bit longer than the reference summary,

2 Example
"""

val_ds[22]

text = "Madison: Hello Lawrence are you through with the article?\r\nLawrence: Not yet sir. \r\nLawrence: But i will be in a few.\r\nMadison: Okay. But make it quick.\r\nMadison: The piece is needed by today\r\nLawrence: Sure thing\r\nLawrence: I will get back to you once i am through."
summary = "Lawrence will finish writing the article soon."
generated_summary = summarizer(text)

print('Original Dialogue:\n')
print(text)
print('\n' * 2)
print('Reference Summary:\n')
print(summary)
print('\n' * 2)
print('Model-generated Summary:\n')
print(generated_summary)

"""Summary is longer than the refernece summary.

*   More informative
*   Sense of urgency

3. Example
"""

val_ds[4]

text = "Robert: Hey give me the address of this music shop you mentioned before\r\nRobert: I have to buy guitar cable\r\nFred: Catch it on google maps\r\nRobert: thx m8\r\nFred: ur welcome"
summary = "Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable."
generated_summary = summarizer(text)

print('Original Dialogue:\n')
print(text)
print('\n' * 2)
print('Reference Summary:\n')
print(summary)
print('\n' * 2)
print('Model-generated Summary:\n')
print(generated_summary)

"""Here , we can observe that the lack of clarity,

*   pronoun 'he ' creates uncertainity whether Fred or Robert intends to buy the gutiar cable.
*   In orignal it is clear that Robert is the one who has to buy the cable.

4. Example - Compare the Summaries, we create some dialogue and input them into the model to check how it performs on them.
"""

# Creating new dialogues for evaluation
text = "John: Hey! I've been thinking about getting a PlayStation 5. Do you think it is worth it? \r\nDan: Idk man. R u sure ur going to have enough free time to play it? \r\nJohn: Yeah, that's why I'm not sure if I should buy one or not. I've been working so much lately idk if I'm gonna be able to play it as much as I'd like."
generated_summary = summarizer(text)

print('Original Dialogue:\n')
print(text)
print('\n' * 2)
print('Model-generated Summary:\n')
print(generated_summary)

"""Including abbrivations such as idk- I don't know and
r u for are you . In conclusion the model successfully capture them

5 Example: Theme of the Competition
"""

text = "Camilla: Who do you think is going to win the competition?\r\nMichelle: I believe Jonathan should win but I'm sure Mike is cheating!\r\nCamilla: Why do you say that? Can you prove Mike is really cheating?\r\nMichelle: I can't! But I just know!\r\nCamilla: You shouldn't accuse him of cheating if you don't have any evidence to support it."
generated_summary = summarizer(text)

print('Original Dialogue:\n')
print(text)
print('\n' * 2)
print('Model-generated Summary:\n')
print(generated_summary)

"""The Model Captures the main theme of the conversation, about Michelle's opinion that Jonathan should win the competition , but Mike may be cheating.

Conclusion

Hence, we use Large Language Modle for several tasks involving Natural Language Processing for Text Summarisation Tasks.

We use BART Model that has trained tp perform summarization on news articles and fine_tuned it to perform summarization of dialogue with the SamSum datasets.
"""